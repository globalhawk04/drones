# FILE: app/services/ai_service.py
import google.generativeai as genai
import json
import re
from app.config import settings
from app.prompts import *

if settings.GOOGLE_API_KEY:
    genai.configure(api_key=settings.GOOGLE_API_KEY)

def parse_json_garbage(text: str) -> dict | None:
    if not text: return None
    match = re.search(r"```(json)?\s*({.*})\s*```", text, re.DOTALL)
    json_str = match.group(2) if match else text
    if not match:
        start, end = text.find("{"), text.rfind("}") + 1
        if start != -1 and end != -1: json_str = text[start:end]
    try:
        return json.loads(json_str)
    except:
        return None

async def call_llm_for_json(prompt: str, system_instruction: str) -> dict | None:
    try:
        model = genai.GenerativeModel('gemini-2.5-pro', system_instruction=system_instruction)
        response = await model.generate_content_async(prompt, generation_config={"response_mime_type": "application/json"})
        return parse_json_garbage(response.text)
    except Exception as e:
        print(f"LLM Error: {e}")
        return None

async def generate_dynamic_buy_list(plan: dict) -> list[str] | None:
    """
    Uses the System Architect AI to generate a dynamic list of required part categories.

    Args:
        plan: The engineering plan containing the 'build_summary'.

    Returns:
        A list of strings, where each string is a required part_type, or None on failure.
    """
    print("--> üèõÔ∏è  System Architect is determining required components...")
    
    build_summary = plan.get("build_summary")
    if not build_summary:
        print("   ‚ùå System Architect: Build summary is missing from the plan.")
        return None

    # The prompt for this AI is just the summary text.
    prompt_content = build_summary

    try:
        # We can't use the standard call_llm_for_json because this prompt returns a raw list, not an object.
        model = genai.GenerativeModel('gemini-2.5-pro', system_instruction=SYSTEM_ARCHITECT_INSTRUCTION)
        response = await model.generate_content_async(prompt_content, generation_config={"response_mime_type": "application/json"})
        
        # Parse the raw JSON list string
        raw_text = response.text.strip()
        
        # Clean up potential markdown formatting
        if raw_text.startswith("```json"):
            raw_text = raw_text[7:-3].strip()
        
        component_list = json.loads(raw_text)

        if isinstance(component_list, list):
            print(f"   ‚úÖ System Architect determined {len(component_list)} component categories are needed.")
            return component_list
        else:
            print("   ‚ùå System Architect: AI did not return a valid list.")
            return None

    except Exception as e:
        print(f"   ‚ùå System Architect Error: {e}")
        return None

async def analyze_user_requirements(user_prompt: str) -> dict:
    print(f"--> üß† Architect Agent Analyzing...")
    return await call_llm_for_json(user_prompt, REQUIREMENTS_SYSTEM_INSTRUCTION)

async def refine_requirements(original_analysis: dict, user_answers: list[str]) -> dict:
    print(f"--> üß† Chief Engineer Refining...")
    context = f"ANALYSIS:\n{json.dumps(original_analysis)}\nANSWERS:\n{json.dumps(user_answers)}"
    final_plan = await call_llm_for_json(context, CONSTRAINT_MERGER_INSTRUCTION)
    
    # --- THE FIX: Inject Topology back into plan ---
    if final_plan and original_analysis.get("topology"):
        final_plan["topology"] = original_analysis["topology"]
    
    return final_plan

async def generate_spec_sheet(plan: dict, dynamic_buy_list: list[str]) -> dict | None:
    """
    Uses the Sourcing Engineer AI to generate search queries for a dynamic list of parts.

    This function is now simpler in its goal but more complex in its context,
    as it must handle dynamic part lists and potential 'forced anchors' for re-designs.

    Args:
        plan: The engineering plan containing the 'build_summary' and an optional 'forced_anchor'.
        dynamic_buy_list: The list of part categories generated by the System Architect.

    Returns:
        A dictionary containing the 'buy_list' with search queries.
    """
    print("--> üìã Sourcing Engineer is generating search queries...")
    
    # 1. Construct the prompt context
    prompt_context = {
        "build_summary": plan.get("build_summary"),
        "dynamic_buy_list": dynamic_buy_list
    }
    # Add the forced_anchor if it exists in the plan (for the "Nuke and Rebuild" strategy)
    if "forced_anchor" in plan:
        prompt_context["forced_anchor"] = plan["forced_anchor"]
        print(f"   -> A 'forced_anchor' is active: Re-planning around {plan['forced_anchor']['part_type']}")


    # 2. Format the context into a JSON string for the prompt
    prompt_content = json.dumps(prompt_context, indent=2)

    # 3. Call the LLM with the new prompt and context
    specs = await call_llm_for_json(prompt_content, SPEC_GENERATOR_INSTRUCTION)

    if not specs or "buy_list" not in specs:
        print("   ‚ùå Sourcing Engineer failed to generate a valid spec sheet.")
        return None
        
    print(f"   ‚úÖ Sourcing Engineer generated {len(specs['buy_list'])} search queries.")
    return specs

async def generate_assembly_instructions(blueprint: dict) -> dict:
    """
    Transforms a validated assembly blueprint into a user-friendly guide.

    This function no longer uses an LLM to generate steps. It directly
    translates the machine-readable blueprint, which has already been
    validated by the main loop, into a human-readable format. This ensures
    the final instructions are 100% consistent with the generated CAD model.

    Args:
        blueprint: The final, validated assembly blueprint dictionary.

    Returns:
        A dictionary containing the guide in Markdown and a simple list of steps.
    """
    print("--> üìù Finalizing documentation from validated blueprint...")

    # The blueprint is the source of truth, so we just need to reformat it.
    steps_list = []
    markdown_lines = ["# Assembly Instructions\n"]

    for step in blueprint.get("blueprint_steps", []):
        # Format for the JSON object used by the dashboard's JavaScript
        steps_list.append({
            "step": step.get("title", "Unnamed Step"),
            "detail": step.get("details", "No details provided.")
        })
        
        # Format for a human-readable Markdown document (optional, but good practice)
        markdown_lines.append(f"### Step {step.get('step_number')}: {step.get('title')}")
        markdown_lines.append(f"{step.get('details')}\n")
        if step.get('fasteners_used'):
            markdown_lines.append(f"**Hardware:** {step.get('fasteners_used')}\n")

    # Add the required fasteners list to the markdown guide
    markdown_lines.append("---")
    markdown_lines.append("\n## Required Hardware\n")
    for fastener in blueprint.get("required_fasteners", []):
        markdown_lines.append(f"- **{fastener.get('item')}** (x{fastener.get('quantity')}) - _{fastener.get('usage')}_")

    final_guide = {
        "guide_md": "\n".join(markdown_lines),
        "steps": steps_list
    }

    return final_guide


async def optimize_specs(current_bom: list, failure_report: dict) -> dict | None:
    """
    Analyzes a design failure and asks the AI to suggest a component replacement.

    This function takes a structured failure report (either 'conceptual' or 'geometric')
    and calls the Optimization Engineer AI to get a machine-readable fix.

    Args:
        current_bom: The Bill of Materials for the failed design.
        failure_report: A dictionary containing the 'type' and 'details' of the failure.

    Returns:
        A dictionary with the AI's suggested fix, or None if the process fails.
    """
    print("--> üîß Optimization Engineer is analyzing the design failure...")

    # 1. Prepare the context for the AI prompt.
    #    This bundles the current component list and the failure report together.
    context = {
        "current_bom": current_bom,
        "failure_report": failure_report
    }
    prompt_content = json.dumps(context, indent=2)

    # 2. Call the LLM with the new, more sophisticated prompt.
    suggested_fix = await call_llm_for_json(prompt_content, OPTIMIZATION_ENGINEER_INSTRUCTION)

    # 3. Robust fallback: If the AI fails to provide a valid fix, return None.
    #    The main loop will need to handle this as a critical, unrecoverable error.
    if not suggested_fix or not suggested_fix.get("replacements"):
        print("   ‚ùå AI Optimization Engineer failed to provide a valid solution.")
        return None

    # 4. Log the AI's thinking process for debugging and transparency.
    print(f"   -> AI Diagnosis: {suggested_fix.get('diagnosis')}")
    print(f"   -> AI Strategy: {suggested_fix.get('strategy')}")

    return suggested_fix

async def generate_assembly_blueprint(bom: list) -> dict:
    """
    Analyzes the BOM for compatibility and generates a machine-readable assembly plan.

    This is the core of the conceptual validation loop. It uses a multimodal AI
    to reason about the physical fitment of parts based on their specs and images.

    Args:
        bom: The complete Bill of Materials list of dictionaries.

    Returns:
        A dictionary conforming to the Assembly Blueprint schema.
    """
    print("--> üß† Master Builder is analyzing component compatibility...")

    # 1. Prepare a simplified, clean version of the BOM for the AI prompt.
    #    We only include the data relevant to the AI's analysis task.
    context_bom = []
    for item in bom:
        context_bom.append({
            "part_type": item.get("part_type"),
            "product_name": item.get("product_name"),
            "engineering_specs": item.get("engineering_specs", {}),
            "reference_image_url": item.get("reference_image")
        })

    # 2. Format the BOM into a JSON string to serve as the main prompt content.
    prompt_content = json.dumps({"bill_of_materials": context_bom}, indent=2)

    # 3. Call the LLM using our helper function and the new, powerful prompt.
    #    The helper handles the API call, error catching, and JSON parsing.
    blueprint = await call_llm_for_json(prompt_content, ASSEMBLY_BLUEPRINT_INSTRUCTION)

    # 4. Implement a robust fallback. If the AI fails or returns invalid data,
    #    we must return a clear "failure" blueprint to prevent the main loop from crashing.
    if not blueprint:
        print("   ‚ùå AI service failed to generate a valid blueprint.")
        return {
            "is_buildable": False,
            "incompatibility_reason": "The AI analysis service failed to return a valid response. This could be due to an API error or invalid output formatting.",
            "required_fasteners": [],
            "blueprint_steps": []
        }

    # 5. If the AI itself determines the build is not buildable, log it.
    if not blueprint.get("is_buildable"):
        reason = blueprint.get("incompatibility_reason", "No reason provided.")
        print(f"   ‚ùå Conceptual Build Failed: {reason}")
    else:
        print("   ‚úÖ Blueprint Generated. Build is conceptually viable.")

    return blueprint

async def ask_for_human_input(plan: dict, failed_item: dict) -> dict:
    """
    When the system is stuck, this function asks the AI to formulate a question for the user.
    """
    print("--> ü§ñ AI is stuck, formulating a question for the user...")
    context = {
        "project_summary": plan.get("build_summary"),
        "failed_sourcing_details": failed_item
    }
    prompt_content = json.dumps(context, indent=2)
    
    # Use a different model if you want, but the main one should be fine
    interaction_data = await call_llm_for_json(prompt_content, HUMAN_INTERACTION_PROMPT)
    
    return interaction_data

async def generate_vision_prompt(part_type: str) -> dict | None:
    """
    Uses the Vision Prompt Engineer AI to dynamically create a vision analysis prompt.

    Args:
        part_type: The component category (e.g., "Companion_Computer").

    Returns:
        A dictionary containing the 'prompt_text' and 'json_schema' for the vision model,
        or None on failure.
    """
    print(f"--> ü§ñ Vision Prompt Engineer is designing a prompt for: {part_type}...")

    # Use the helper function that expects a JSON object as a response.
    # The prompt content is just the name of the part type.
    prompt_object = await call_llm_for_json(part_type, VISION_PROMPT_ENGINEER_INSTRUCTION)

    if not prompt_object or "prompt_text" not in prompt_object or "json_schema" not in prompt_object:
        print(f"   ‚ùå Vision Prompt Engineer failed to generate a valid prompt object for {part_type}.")
        return None
    
    print(f"   ‚úÖ Vision prompt for {part_type} generated successfully.")
    return prompt_object